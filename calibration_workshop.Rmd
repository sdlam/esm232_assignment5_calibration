---
title: "Calibration Workshop"
author: "Zoe Rennie, Sarah Lam"
date: "2023-05-09"
output:
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(dplyr.summarise.inform = FALSE)
library(tidyverse)
library(here)
```

### Function output using combined performance metric on Sagehen data

```{r}
sager <- read.table(here("data", "sager.txt"), header=T)
sager = sager %>% mutate(date = paste(day,month,year, sep="/"))
sager$date = as.Date(sager$date,"%d/%m/%Y")

mme_corcoeff <- function(obs, model, wy, month){
new_frame <- cbind.data.frame(obs,model, wy, month)
monthly_means <- new_frame %>%
  #group by water year and month
  group_by(wy, month) %>%
  # take mean of model est. and observed values by month by water year
 summarize(mean_model=mean(model), mean_obs=mean(obs))
mean_monthly_error <- monthly_means %>%
  #take difference between mean model and mean obs values
  mutate(diff=mean_model-mean_obs) %>%
  ungroup()
mean_monthly_error <- abs(mean(mean_monthly_error$diff))
correlation <- cor(monthly_means$mean_model, monthly_means$mean_obs)
combined = .5*mean_monthly_error+ .5*correlation
return(combined)
}
mme_corcoeff(sager$obs, sager$model, sager$wy, sager$month)
```

We chose this metric because firstly, average monthly error seemed like an intuitive and interesting way to compare the accuracy of the model's predictions against observed values. Moreover, we felt that the correlation between the model generated and observed values would also serve to represent how accurately the model was able to predict values. Thus, we thought it was logical to combine these two metrics to form a performance metric in order to assess this model.

### Apply function to msage set

```{r}
#Calibration
msage = read.table(here("data","sagerm.txt"), header=T)
nsim = ncol(msage)
snames = sprintf("S%d",seq(from=1, to=nsim))
colnames(msage)=snames
msage$date = sager$date
msage$month = sager$month
msage$year = sager$year
msage$day = sager$day
msage$wy = sager$wy
msage = left_join(msage, sager[,c("obs","date")], by=c("date"))
msagel = msage %>% pivot_longer(cols=!c(date, month, year, day,wy), names_to="run", values_to="flow")

short_msage = subset(msage, wy < 1975)
# compute performance measures for output from all parameters
res = short_msage %>% select(!c("date","month","year","day","wy","obs")) %>%
      map_dbl(mme_corcoeff, short_msage$obs, short_msage$wy, short_msage$month)

head(res)
best <- res[which.max(res)]
worst <- res[which.min(res)]
```

##### Best parameter set
```{r}
best
res_df = short_msage %>% select(!c("date","month","year","day","wy","obs")) %>%
      map_df(mme_corcoeff, short_msage$obs, short_msage$wy, short_msage$month)
```


##### Worst parameter set
```{r}
worst
```

### Graph of performance over calibration period
```{r}
# graph results of best model compared to observed data

ggplot(msage, aes(date, S95))+ geom_line(aes(col = "model"))+
  geom_line(aes(date, obs, col = "observed")) +
  labs(x = "Date", 
       y = "Stream Flow mm/day") + ggtitle("Best Performing Parameter Set S95") + theme_minimal()
```

### Extra Credit Section

#### Select acceptable outcomes
```{r}
#convert to data frame
res_2 <- as.data.frame(res)
res_df <- cbind.data.frame(res=res_2$res, sim=paste0("S", 1:101))

#reorder from highest to lowest 
tmp =  res_df %>% arrange(desc(res))

#select the highest 50 
res_acc=head(tmp, n=50) 
best_sims <- res_acc$sim
acceptable_short_msage <- short_msage %>% select(all_of(best_sims), date, month, year, day, wy, obs)
head(acceptable_short_msage)
```

##### Compute Range
```{r}
res_range = acceptable_short_msage %>% select(-date, -month, -day, -year, -wy, -obs ) %>%
  map_df(mme_corcoeff, short_msage$obs, short_msage$wy, short_msage$month) %>% gather(sim, values)
summary(res_range)
```

#### Graph range
```{r}
#graph range of performance measures
ggplot(res_range, aes("", values))+geom_boxplot() + ggtitle("Graph of range of outcomes for acceptable parameters")+ylab("Range of outcomes") + theme_minimal()
```

#### Compute and graph maximum liklihood estimate
```{r}
sum_acc=sum(res_acc$res)
res_acc$wt_acc=res_acc$res/sum_acc

msagel  =  msage %>% pivot_longer(cols=!c(date, month, year, day,wy, obs), names_to="sim", values_to="flow")

#subset only acceptable runs
msagel_acc = subset(msagel, sim %in% res_acc$sim)
#join with weights from res_acc
msagel_acc = left_join(msagel_acc, res_acc, by="sim")
head(msagel_acc)
# finally multiply flow by weight
msagel_acc = msagel_acc %>% mutate(flow_wt = flow*wt_acc)

#average streamflow for each day from all the runs
aver_flow = msagel_acc %>% group_by(date) %>% dplyr::summarize(meanstr = sum(flow_wt))
head(aver_flow)

ggplot(aver_flow, aes(x=date, y=meanstr))+geom_line(col="red")+labs(y="Streamflow mm/day") + ggtitle("Graph of Likelihood Estimate") + theme_minimal()

# add some of the other date info and plot a subset
aver_flow$wy = msage$wy
wycheck=1985
ggplot(subset(aver_flow, wy == wycheck), aes(x=date, y=meanstr, col="model_wt"))+
  geom_line()+labs(y="Streamflow mm/day")+
  geom_line(data=subset(msage, wy==wycheck), aes(date, obs, col="obs")) + theme_minimal()+ ggtitle("Graph of Subset of Likelihood Estimate")
```

We decided to choose the best 50 outcomes since it seemed more straightforward than setting an arbitrary threshold for the cutoff of performance. Based on our visualizations, specifically our maximum likelihood graphs, it looks like our model might underestimate the flow somewhat but it's relatively close at least to the global maximum of the observed. We think based on the proximity of the model's estimates to the observed, that our combined performance did a fair job at capturing the variation between the model and observed values, although it definitely has room for improvement, as it is not a precise match.